{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlH6qp6QMQoKJy1nMOHGzA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nell87/drivendata_richter/blob/main/script/analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Overview**\n",
        "\n",
        "Based on aspects of building location and construction, **our goal is to predict the level of damage to buildings** caused by the 2015 Gorkha earthquake in Nepal.\n",
        "\n",
        "The data was collected through surveys by Kathmandu Living Labs and the Central Bureau of Statistics, which works under the National Planning Commission Secretariat of Nepal. This survey is one of the largest post-disaster datasets ever collected, containing valuable information on earthquake impacts, household conditions, and socio-economic-demographic statistics."
      ],
      "metadata": {
        "id": "4yUhJmgAMtrl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "meJDvOtLHB6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0602391e-45ac-4485-da00-2d3cea3e5b4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(260601, 39)\n"
          ]
        }
      ],
      "source": [
        "####    INCLUDES  _______________________________________ #### \n",
        "#Loading Libraries:# \n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "####    READING TRAIN AND TEST DATA _______________________________________ #### \n",
        "train= data = pd.read_csv(\"https://raw.githubusercontent.com/Nell87/drivendata_richter/main/data/train_values.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/Nell87/drivendata_richter/main/data/test_values.csv\")\n",
        "print(train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EDA**\n",
        "Let's start checking training and test data similarity (if they are not, the performance on test could be really bad)"
      ],
      "metadata": {
        "id": "VIjZn9W4ODld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn.preprocessing import StandardScaler # for preprocessing the data\n",
        "from sklearn.pipeline import make_pipeline # for combining the preprocess with model training\n",
        "from sklearn.model_selection import GridSearchCV # for optimizing the hyperparameters of the pipeline\n",
        "\n",
        "# We add the label 1 (train) and 0 (test) and merge both datasets\n",
        "train[\"check\"]=1\n",
        "test[\"check\"]=0\n",
        "merged_data = pd.concat([train,test], axis=0)\n",
        "merged_data[['check', 'land_surface_condition', 'foundation_type', 'roof_type', 'ground_floor_type','other_floor_type', 'position', 'plan_configuration', 'legal_ownership_status']] = merged_data[['check', 'land_surface_condition', 'foundation_type', 'roof_type', 'ground_floor_type','other_floor_type', 'position', 'plan_configuration', 'legal_ownership_status']].apply(lambda x: x.astype('category'))\n",
        "\n",
        "# We split dataset in features and target variable\n",
        "x= merged_data.iloc[:, :39]\n",
        "y= merged_data.iloc[:,39:]\n",
        "\n",
        "# Create train and test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
        "x_train = pd.get_dummies(x_train)\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "clf.fit(x_train,y_train.ravel())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5M7A1KsTWTxB",
        "outputId": "c3f25ce0-322d-434e-a4ff-c03557493a75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0d8eb6589441>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#Train the model using the training sets y_pred=clf.predict(X_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'ravel'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = pd.read_csv(\"https://raw.githubusercontent.com/Nell87/drivendata_richter/main/data/train_labels.csv\")\n",
        "train = train.merge(train_labels, on='building_id')"
      ],
      "metadata": {
        "id": "nQOeCbcSMqVu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}