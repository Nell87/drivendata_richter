{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcREhuaLTDX2gBDP9X6ktD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nell87/drivendata_richter/blob/main/script/analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Overview**\n",
        "\n",
        "Based on aspects of building location and construction, **our goal is to predict the level of damage to buildings** caused by the 2015 Gorkha earthquake in Nepal.\n",
        "\n",
        "The data was collected through surveys by Kathmandu Living Labs and the Central Bureau of Statistics, which works under the National Planning Commission Secretariat of Nepal. This survey is one of the largest post-disaster datasets ever collected, containing valuable information on earthquake impacts, household conditions, and socio-economic-demographic statistics."
      ],
      "metadata": {
        "id": "4yUhJmgAMtrl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "meJDvOtLHB6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eca7f11-dac8-4149-9958-47c1f7d1dd01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(260601, 39)\n"
          ]
        }
      ],
      "source": [
        "####    INCLUDES  _______________________________________ #### \n",
        "#Loading Libraries:# \n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "####    READING TRAIN AND TEST DATA _______________________________________ #### \n",
        "train= data = pd.read_csv(\"https://raw.githubusercontent.com/Nell87/drivendata_richter/main/data/train_values.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/Nell87/drivendata_richter/main/data/test_values.csv\")\n",
        "print(train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EDA**\n",
        "Let's start checking training and test data similarity (if they are not, the performance on test could be really bad)"
      ],
      "metadata": {
        "id": "VIjZn9W4ODld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
        "from imblearn.pipeline import make_pipeline as imb_make_pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# We add the label 1 (train) and 0 (test) and merge both datasets\n",
        "train[\"check\"]=1\n",
        "test[\"check\"]=0\n",
        "merged_data = pd.concat([train,test], axis=0)\n",
        "merged_data[['check', 'land_surface_condition', 'foundation_type', 'roof_type', 'ground_floor_type','other_floor_type', 'position', 'plan_configuration', 'legal_ownership_status']] = merged_data[['check', 'land_surface_condition', 'foundation_type', 'roof_type', 'ground_floor_type','other_floor_type', 'position', 'plan_configuration', 'legal_ownership_status']].apply(lambda x: x.astype('category'))\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "scaled_df = merged_data.copy()\n",
        "\n",
        "# scaling features\n",
        "col = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id', 'age', 'area_percentage','count_floors_pre_eq' ,'height_percentage']\n",
        "features = scaled_df[col]\n",
        "features = std_scaler.fit_transform(features.values)\n",
        "scaled_df[col] = features\n",
        "\n",
        "x = pd.get_dummies(scaled_df.drop('check', axis=1))\n",
        "y = scaled_df.check\n",
        "\n",
        "#x= merged_data.iloc[:, :39]\n",
        "#y= merged_data.iloc[:,39:]\n",
        "\n",
        "# Create train and test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
        "\n",
        "rf_clf = RandomForestClassifier() \n",
        "\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "rf_clf.fit(x_train,y_train)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5M7A1KsTWTxB",
        "outputId": "d607977f-f4d9-4604-bc8c-787d12726c97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=rf_clf.predict(x_test)\n"
      ],
      "metadata": {
        "id": "00mHxnq_FxZ4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "5Q5aSDl8FZOn",
        "outputId": "53ee3784-e283-4ecc-ebba-1b4a2a79e38a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7389414913517713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = pd.read_csv(\"https://raw.githubusercontent.com/Nell87/drivendata_richter/main/data/train_labels.csv\")\n",
        "train = train.merge(train_labels, on='building_id')"
      ],
      "metadata": {
        "id": "nQOeCbcSMqVu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}